{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import dateparser\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from cleantext import clean\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import swifter\n",
    "from somajo import Tokenizer, SentenceSplitter\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from german_lemmatizer import lemmatize\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/mnt/data2/ptf/cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slem_txt = list(lemmatize(df['text'].values[0:], n_jobs=3, chunk_size=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( slem_txt, open( \"lemma.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slem_txt = pickle.load( open( \"lemma.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sents(texts):\n",
    "    tokenizer = Tokenizer(split_camel_case=True, token_classes=False, extra_info=False)\n",
    "    sentence_splitter = SentenceSplitter(is_tuple=False)\n",
    "    \n",
    "    results = []\n",
    "    for text in texts:\n",
    "#         text = clean(text, lang='de', lower=False)\n",
    "        tokens = tokenizer.tokenize_paragraph(text)\n",
    "        sentences = sentence_splitter.split(tokens)\n",
    "        cleaned = [clean(' '.join(s), no_urls=True, no_digits=True, no_punct=True, no_line_breaks=True, lang='de') for s in sentences]\n",
    "        results.append(cleaned)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=4)(delayed(get_sents)(row) for row in tqdm(list(chunks(slem_txt, 1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load( open( \"results_lemma.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/data2/ptf/zo2.txt', 'w') as outfile:\n",
    "    for r in results:\n",
    "        for d in r:\n",
    "            outfile.write('\\n'.join([ x for x in d]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( results, open( \"results_lemma.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slem_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fucked up, need to either remove new lines or escape them for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results[:100]:\n",
    "    for d in r:\n",
    "        print(\"\\n\".join(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
